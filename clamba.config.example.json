{
  "llm_provider": "ollama",
  "llm_endpoint": "http://localhost:11434/api/generate",
  "model_name": "mistral",
  "temperature": 0.3,
  "max_tokens": 1000
}
